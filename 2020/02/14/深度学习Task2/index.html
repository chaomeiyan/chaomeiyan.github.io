<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Maxwell" />



<meta name="description" content="文本预处理文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：  读入文本 分词 建立字典，将每个词映射到一个唯一的索引（index） 将文本从词的序列转换为索引的序列，方便输入模型  读入文本我们用一部英文小说，即H. G. Well的Time Machine，作为示例，展示文本预处理的具体过程。 123456789101112im">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习Task2">
<meta property="og:url" content="https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/index.html">
<meta property="og:site_name" content="Maxwell&#39;s Blog">
<meta property="og:description" content="文本预处理文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：  读入文本 分词 建立字典，将每个词映射到一个唯一的索引（index） 将文本从词的序列转换为索引的序列，方便输入模型  读入文本我们用一部英文小说，即H. G. Well的Time Machine，作为示例，展示文本预处理的具体过程。 123456789101112im">
<meta property="og:image" content="https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213233029.png">
<meta property="og:image" content="https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213249636.png">
<meta property="og:image" content="https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213504355.png">
<meta property="article:published_time" content="2020-02-14T13:27:19.000Z">
<meta property="article:modified_time" content="2020-02-14T13:39:50.510Z">
<meta property="article:author" content="Maxwell">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213233029.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Maxwell&#39;s Blog" type="application/atom+xml">



    <link rel="shortcut icon" href="/apple-touch-icon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>深度学习Task2 | Maxwell&#39;s Blog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/me.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Maxwell</a></h1>
        </hgroup>

        
        <p class="header-subtitle">中国科学技术大学计算机学院大二在读</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="https://mail.ustc.edu.cn/" target="_blank" rel="noopener" title="Email"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" href="https://www.zhihu.com/people/yan-chao-mei-53" target="_blank" rel="noopener" title="知乎"></a>
                            
                                <a class="fa CSDN" href="https://mp.csdn.net/" target="_blank" rel="noopener" title="CSDN"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACM/" rel="tag">ACM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7/" rel="tag">工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%84%9F%E6%83%B3/" rel="tag">感想</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" rel="tag">编程语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%AE%E9%A1%B6/" rel="tag">置顶</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">中国科学技术大学计算机学院大二在读</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Maxwell</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/me.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Maxwell</a></h1>
            </hgroup>
            
            <p class="header-subtitle">中国科学技术大学计算机学院大二在读</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="https://mail.ustc.edu.cn/" title="Email"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" target="_blank" href="https://www.zhihu.com/people/yan-chao-mei-53" title="知乎"></a>
                            
                                <a class="fa CSDN" target="_blank" href="https://mp.csdn.net/" title="CSDN"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-深度学习Task2" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/" class="article-date">
      <time datetime="2020-02-14T13:27:19.000Z" itemprop="datePublished">2020-02-14</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习Task2
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        

        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<ol>
<li>读入文本</li>
<li>分词</li>
<li>建立字典，将每个词映射到一个唯一的索引（index）</li>
<li>将文本从词的序列转换为索引的序列，方便输入模型</li>
</ol>
<h4 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h4><p>我们用一部英文小说，即H. G. Well的<a href="http://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener">Time Machine</a>，作为示例，展示文本预处理的具体过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import collections</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">def read_time_machine():</span><br><span class="line">    with open(&#39;&#x2F;home&#x2F;kesci&#x2F;input&#x2F;timemachine7163&#x2F;timemachine.txt&#39;, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; [re.sub(&#39;[^a-z]+&#39;, &#39; &#39;, line.strip().lower()) for line in f]</span><br><span class="line">    return lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines &#x3D; read_time_machine()</span><br><span class="line">print(&#39;# sentences %d&#39; % len(lines))</span><br><span class="line"># sentences 3221</span><br></pre></td></tr></table></figure>

<h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(sentences, token&#x3D;&#39;word&#39;):</span><br><span class="line">    &quot;&quot;&quot;Split sentences into word or char tokens&quot;&quot;&quot;</span><br><span class="line">    if token &#x3D;&#x3D; &#39;word&#39;:</span><br><span class="line">        return [sentence.split(&#39; &#39;) for sentence in sentences]</span><br><span class="line">    elif token &#x3D;&#x3D; &#39;char&#39;:</span><br><span class="line">        return [list(sentence) for sentence in sentences]</span><br><span class="line">    else:</span><br><span class="line">        print(&#39;ERROR: unkown token type &#39;+token)</span><br><span class="line"></span><br><span class="line">tokens &#x3D; tokenize(lines)</span><br><span class="line">tokens[0:2]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;, &#39;&#39;], [&#39;&#39;]]</span><br></pre></td></tr></table></figure>

<h4 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h4><p>为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class Vocab(object):</span><br><span class="line">    def __init__(self, tokens, min_freq&#x3D;0, use_special_tokens&#x3D;False):</span><br><span class="line">        counter &#x3D; count_corpus(tokens)  # : </span><br><span class="line">        self.token_freqs &#x3D; list(counter.items())</span><br><span class="line">        self.idx_to_token &#x3D; []</span><br><span class="line">        if use_special_tokens:</span><br><span class="line">            # padding, begin of sentence, end of sentence, unknown</span><br><span class="line">            self.pad, self.bos, self.eos, self.unk &#x3D; (0, 1, 2, 3)</span><br><span class="line">            self.idx_to_token +&#x3D; [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;]</span><br><span class="line">        else:</span><br><span class="line">            self.unk &#x3D; 0</span><br><span class="line">            self.idx_to_token +&#x3D; [&#39;&#39;]</span><br><span class="line">        self.idx_to_token +&#x3D; [token for token, freq in self.token_freqs</span><br><span class="line">                        if freq &gt;&#x3D; min_freq and token not in self.idx_to_token]</span><br><span class="line">        self.token_to_idx &#x3D; dict()</span><br><span class="line">        for idx, token in enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] &#x3D; idx</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, tokens):</span><br><span class="line">        if not isinstance(tokens, (list, tuple)):</span><br><span class="line">            return self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        return [self.__getitem__(token) for token in tokens]</span><br><span class="line"></span><br><span class="line">    def to_tokens(self, indices):</span><br><span class="line">        if not isinstance(indices, (list, tuple)):</span><br><span class="line">            return self.idx_to_token[indices]</span><br><span class="line">        return [self.idx_to_token[index] for index in indices]</span><br><span class="line"></span><br><span class="line">def count_corpus(sentences):</span><br><span class="line">    tokens &#x3D; [tk for st in sentences for tk in st]</span><br><span class="line">    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数</span><br></pre></td></tr></table></figure>

<p>我们看一个例子，这里我们尝试用Time Machine作为语料构建字典</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab &#x3D; Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[0:10])</span><br><span class="line">[(&#39;&#39;, 0), (&#39;the&#39;, 1), (&#39;time&#39;, 2), (&#39;machine&#39;, 3), (&#39;by&#39;, 4), (&#39;h&#39;, 5), (&#39;g&#39;, 6), (&#39;wells&#39;, 7), (&#39;i&#39;, 8), (&#39;traveller&#39;, 9)]</span><br></pre></td></tr></table></figure>

<h4 id="将词转为索引"><a href="#将词转为索引" class="headerlink" title="将词转为索引"></a>将词转为索引</h4><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i in range(8, 10):</span><br><span class="line">    print(&#39;words:&#39;, tokens[i])</span><br><span class="line">    print(&#39;indices:&#39;, vocab[tokens[i]])</span><br><span class="line">words: [&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;, &#39;for&#39;, &#39;so&#39;, &#39;it&#39;, &#39;will&#39;, &#39;be&#39;, &#39;convenient&#39;, &#39;to&#39;, &#39;speak&#39;, &#39;of&#39;, &#39;him&#39;, &#39;&#39;]</span><br><span class="line">indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]</span><br><span class="line">words: [&#39;was&#39;, &#39;expounding&#39;, &#39;a&#39;, &#39;recondite&#39;, &#39;matter&#39;, &#39;to&#39;, &#39;us&#39;, &#39;his&#39;, &#39;grey&#39;, &#39;eyes&#39;, &#39;shone&#39;, &#39;and&#39;]</span><br><span class="line">indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]</span><br></pre></td></tr></table></figure>

<h4 id="用现有工具进行分词"><a href="#用现有工具进行分词" class="headerlink" title="用现有工具进行分词"></a>用现有工具进行分词</h4><p>我们前面介绍的分词方式非常简单，它至少有以下几个缺点:</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<p>下面是一个简单的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text &#x3D; &quot;Mr. Chen doesn&#39;t agree with my suggestion.&quot;</span><br></pre></td></tr></table></figure>

<p>spaCy:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">nlp &#x3D; spacy.load(&#39;en_core_web_sm&#39;)</span><br><span class="line">doc &#x3D; nlp(text)</span><br><span class="line">print([token.text for token in doc])</span><br><span class="line">[&#39;Mr.&#39;, &#39;Chen&#39;, &#39;does&#39;, &quot;n&#39;t&quot;, &#39;agree&#39;, &#39;with&#39;, &#39;my&#39;, &#39;suggestion&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure>

<p>NLTK:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">from nltk import data</span><br><span class="line">data.path.append(&#39;&#x2F;home&#x2F;kesci&#x2F;input&#x2F;nltk_data3784&#x2F;nltk_data&#39;)</span><br><span class="line">print(word_tokenize(text))</span><br><span class="line">[&#39;Mr.&#39;, &#39;Chen&#39;, &#39;does&#39;, &quot;n&#39;t&quot;, &#39;agree&#39;, &#39;with&#39;, &#39;my&#39;, &#39;suggestion&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure>

<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为<em>T</em>的文本中的词依次为<em>w</em>1,<em>w</em>2,…,<em>w*</em>T<em>，那么在离散的时间序列中，</em>w<strong>t<em>（1≤</em>t<em>≤</em>T<em>）可看作在时间步（time step）</em>t<em>的输出或标签。给定一个长度为</em>T<em>的词的序列</em>w<em>1,</em>w<em>2,…,</em>w</strong>T*，语言模型将计算该序列的概率：</p>
<p><em>P</em>(<em>w</em>1,<em>w</em>2,…,<em>w**T</em>).</p>
<p>语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。</p>
<h4 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h4><p><img src="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213233029.png" alt="image-20200214213233029"></p>
<h4 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h4><p><img src="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213249636.png" alt="image-20200214213249636"></p>
<h3 id="循环神经网络基础"><a href="#循环神经网络基础" class="headerlink" title="循环神经网络基础"></a>循环神经网络基础</h3><p>上一节介绍的nn元语法中，时间步tt的词wtwt基于前面所有词的条件概率只考虑了最近时间步的n−1n−1个词。如果要考虑比t−(n−1)t−(n−1)更早时间步的词对wtwt的可能影响，我们需要增大nn。但这样模型参数的数量将随之呈指数级增长（可参考上一节的练习）。</p>
<p>本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p>
<h4 id="循环网络的构造"><a href="#循环网络的构造" class="headerlink" title="循环网络的构造"></a>循环网络的构造</h4><p><img src="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2%5Cimage-20200214213504355.png" alt="image-20200214213504355"></p>
<h4 id="从零开始实现循环神经网络"><a href="#从零开始实现循环神经网络" class="headerlink" title="从零开始实现循环神经网络"></a>从零开始实现循环神经网络</h4><p>我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据：</p>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import time</span><br><span class="line">import math</span><br><span class="line">import sys</span><br><span class="line">sys.path.append(&quot;&#x2F;home&#x2F;kesci&#x2F;input&quot;)</span><br><span class="line">import d2l_jay9460 as d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) &#x3D; d2l.load_data_jay_lyrics()</span><br><span class="line">device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</span><br></pre></td></tr></table></figure>

<h5 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h5><p>我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是NN，每次字符对应一个从00到N−1N−1的唯一的索引，则该字符的向量是一个长度为NN的向量，若字符的索引是ii，则该向量的第ii个位置为11，其他位置为00。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>
<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def one_hot(x, n_class, dtype&#x3D;torch.float32):</span><br><span class="line">    result &#x3D; torch.zeros(x.shape[0], n_class, dtype&#x3D;dtype, device&#x3D;x.device)  # shape: (n, n_class)</span><br><span class="line">    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] &#x3D; 1</span><br><span class="line">    return result</span><br><span class="line">    </span><br><span class="line">x &#x3D; torch.tensor([0, 2])</span><br><span class="line">x_one_hot &#x3D; one_hot(x, vocab_size)</span><br><span class="line">print(x_one_hot)</span><br><span class="line">print(x_one_hot.shape)</span><br><span class="line">print(x_one_hot.sum(axis&#x3D;1))</span><br><span class="line">tensor([[1., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1.,  ..., 0., 0., 0.]])</span><br><span class="line">torch.Size([2, 1027])</span><br><span class="line">tensor([1., 1.])</span><br></pre></td></tr></table></figure>

<p>我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步tt的输入为Xt∈Rn×dXt∈Rn×d，其中nn为批量大小，dd为词向量大小，即one-hot向量长度（词典大小）。</p>
<p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def to_onehot(X, n_class):</span><br><span class="line">    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]</span><br><span class="line"></span><br><span class="line">X &#x3D; torch.arange(10).view(2, 5)</span><br><span class="line">inputs &#x3D; to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[0].shape)</span><br><span class="line">5 torch.Size([2, 1027])</span><br></pre></td></tr></table></figure>

<h5 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h5><p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs &#x3D; vocab_size, 256, vocab_size</span><br><span class="line"># num_inputs: d</span><br><span class="line"># num_hiddens: h, 隐藏单元的个数是超参数</span><br><span class="line"># num_outputs: q</span><br><span class="line"></span><br><span class="line">def get_params():</span><br><span class="line">    def _one(shape):</span><br><span class="line">        param &#x3D; torch.zeros(shape, device&#x3D;device, dtype&#x3D;torch.float32)</span><br><span class="line">        nn.init.normal_(param, 0, 0.01)</span><br><span class="line">        return torch.nn.Parameter(param)</span><br><span class="line"></span><br><span class="line">    # 隐藏层参数</span><br><span class="line">    W_xh &#x3D; _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh &#x3D; _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h &#x3D; torch.nn.Parameter(torch.zeros(num_hiddens, device&#x3D;device))</span><br><span class="line">    # 输出层参数</span><br><span class="line">    W_hq &#x3D; _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q &#x3D; torch.nn.Parameter(torch.zeros(num_outputs, device&#x3D;device))</span><br><span class="line">    return (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>

<h5 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h5><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>
<p>In [5]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rnn(inputs, state, params):</span><br><span class="line">    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q &#x3D; params</span><br><span class="line">    H, &#x3D; state</span><br><span class="line">    outputs &#x3D; []</span><br><span class="line">    for X in inputs:</span><br><span class="line">        H &#x3D; torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y &#x3D; torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return outputs, (H,)</span><br></pre></td></tr></table></figure>

<p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p>
<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def init_rnn_state(batch_size, num_hiddens, device):</span><br><span class="line">    return (torch.zeros((batch_size, num_hiddens), device&#x3D;device), )</span><br></pre></td></tr></table></figure>

<p>做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。</p>
<p>In [7]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape)</span><br><span class="line">print(num_hiddens)</span><br><span class="line">print(vocab_size)</span><br><span class="line">state &#x3D; init_rnn_state(X.shape[0], num_hiddens, device)</span><br><span class="line">inputs &#x3D; to_onehot(X.to(device), vocab_size)</span><br><span class="line">params &#x3D; get_params()</span><br><span class="line">outputs, state_new &#x3D; rnn(inputs, state, params)</span><br><span class="line">print(len(inputs), inputs[0].shape)</span><br><span class="line">print(len(outputs), outputs[0].shape)</span><br><span class="line">print(len(state), state[0].shape)</span><br><span class="line">print(len(state_new), state_new[0].shape)</span><br><span class="line">torch.Size([2, 5])</span><br><span class="line">256</span><br><span class="line">1027</span><br><span class="line">5 torch.Size([2, 1027])</span><br><span class="line">5 torch.Size([2, 1027])</span><br><span class="line">1 torch.Size([2, 256])</span><br><span class="line">1 torch.Size([2, 256])</span><br></pre></td></tr></table></figure>

<h5 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h5><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 gg，并设裁剪的阈值是θθ。裁剪后的梯度</p>
<p>min(θ∥g∥,1)gmin(θ‖g‖,1)g</p>
<p>的L2L2范数不超过θθ。</p>
<p>In [8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def grad_clipping(params, theta, device):</span><br><span class="line">    norm &#x3D; torch.tensor([0.0], device&#x3D;device)</span><br><span class="line">    for param in params:</span><br><span class="line">        norm +&#x3D; (param.grad.data ** 2).sum()</span><br><span class="line">    norm &#x3D; norm.sqrt().item()</span><br><span class="line">    if norm &gt; theta:</span><br><span class="line">        for param in params:</span><br><span class="line">            param.grad.data *&#x3D; (theta &#x2F; norm)</span><br></pre></td></tr></table></figure>

<h5 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h5><p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。这个函数稍显复杂，其中我们将循环神经单元<code>rnn</code>设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</p>
<p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,</span><br><span class="line">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):</span><br><span class="line">    state &#x3D; init_rnn_state(1, num_hiddens, device)</span><br><span class="line">    output &#x3D; [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符</span><br><span class="line">    for t in range(num_chars + len(prefix) - 1):</span><br><span class="line">        # 将上一时间步的输出作为当前时间步的输入</span><br><span class="line">        X &#x3D; to_onehot(torch.tensor([[output[-1]]], device&#x3D;device), vocab_size)</span><br><span class="line">        # 计算输出和更新隐藏状态</span><br><span class="line">        (Y, state) &#x3D; rnn(X, state, params)</span><br><span class="line">        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span><br><span class="line">        if t &lt; len(prefix) - 1:</span><br><span class="line">            output.append(char_to_idx[prefix[t + 1]])</span><br><span class="line">        else:</span><br><span class="line">            output.append(Y[0].argmax(dim&#x3D;1).item())</span><br><span class="line">    return &#39;&#39;.join([idx_to_char[i] for i in output])</span><br></pre></td></tr></table></figure>

<p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p>
<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_rnn(&#39;分开&#39;, 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span><br><span class="line">            device, idx_to_char, char_to_idx)</span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;分开濡时食提危踢拆田唱母&#39;</span><br></pre></td></tr></table></figure>

<h5 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h5><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下<a href="https://staticcdn.boyuai.com/course/jupyter/chapter_deep-learning-basics/softmax-regression.ipynb" target="_blank" rel="noopener">“softmax回归”</a>一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小<code>vocab_size</code>。</p>
<h5 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h5><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>
<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li>
</ol>
<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span><br><span class="line">                          lr, clipping_theta, batch_size, pred_period,</span><br><span class="line">                          pred_len, prefixes):</span><br><span class="line">    if is_random_iter:</span><br><span class="line">        data_iter_fn &#x3D; d2l.data_iter_random</span><br><span class="line">    else:</span><br><span class="line">        data_iter_fn &#x3D; d2l.data_iter_consecutive</span><br><span class="line">    params &#x3D; get_params()</span><br><span class="line">    loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态</span><br><span class="line">            state &#x3D; init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start &#x3D; 0.0, 0, time.time()</span><br><span class="line">        data_iter &#x3D; data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        for X, Y in data_iter:</span><br><span class="line">            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态</span><br><span class="line">                state &#x3D; init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            else:  # 否则需要使用detach函数从计算图分离隐藏状态</span><br><span class="line">                for s in state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">            inputs &#x3D; to_onehot(X, vocab_size)</span><br><span class="line">            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">            (outputs, state) &#x3D; rnn(inputs, state, params)</span><br><span class="line">            # 拼接之后形状为(num_steps * batch_size, vocab_size)</span><br><span class="line">            outputs &#x3D; torch.cat(outputs, dim&#x3D;0)</span><br><span class="line">            # Y的形状是(batch_size, num_steps)，转置后再变成形状为</span><br><span class="line">            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span><br><span class="line">            y &#x3D; torch.flatten(Y.T)</span><br><span class="line">            # 使用交叉熵损失计算平均分类误差</span><br><span class="line">            l &#x3D; loss(outputs, y.long())</span><br><span class="line">            </span><br><span class="line">            # 梯度清0</span><br><span class="line">            if params[0].grad is not None:</span><br><span class="line">                for param in params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  # 裁剪梯度</span><br><span class="line">            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均</span><br><span class="line">            l_sum +&#x3D; l.item() * y.shape[0]</span><br><span class="line">            n +&#x3D; y.shape[0]</span><br><span class="line"></span><br><span class="line">        if (epoch + 1) % pred_period &#x3D;&#x3D; 0:</span><br><span class="line">            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (</span><br><span class="line">                epoch + 1, math.exp(l_sum &#x2F; n), time.time() - start))</span><br><span class="line">            for prefix in prefixes:</span><br><span class="line">                print(&#39; -&#39;, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>

<h5 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h5><p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta &#x3D; 250, 35, 32, 1e2, 1e-2</span><br><span class="line">pred_period, pred_len, prefixes &#x3D; 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]</span><br></pre></td></tr></table></figure>

<p>下面采用随机采样训练模型并创作歌词。</p>
<p>In [13]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, True, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line">epoch 50, perplexity 65.808092, time 0.78 sec</span><br><span class="line"> - 分开 我想要这样 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我</span><br><span class="line"> - 不分开 别颗去 一颗两 三颗四 一颗四 三颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一</span><br><span class="line">epoch 100, perplexity 9.794889, time 0.72 sec</span><br><span class="line"> - 分开 一直在美留 谁在它停 在小村外的溪边 默默等  什么 旧你在依旧 我有儿有些瘦 世色我遇见你是一场</span><br><span class="line"> - 不分开吗 我不能再想 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 </span><br><span class="line">epoch 150, perplexity 2.772557, time 0.80 sec</span><br><span class="line"> - 分开 有直在不妥 有话它停留 蜥蝪横怕落 不爽就 旧怪堂 是属于依 心故之 的片段 有一些风霜 老唱盘 </span><br><span class="line"> - 不分开吗 然后将过不 我慢 失些  如  静里回的太快 想通 却又再考倒我 说散 你想很久了吧?的我 从等</span><br><span class="line">epoch 200, perplexity 1.601744, time 0.73 sec</span><br><span class="line"> - 分开 那只都它满在我面妈 捏成你的形状啸而过 或愿说在后能 让梭时忆对着轻轻 我想就这样牵着你的手不放开</span><br><span class="line"> - 不分开期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 不是</span><br><span class="line">epoch 250, perplexity 1.323342, time 0.78 sec</span><br><span class="line"> - 分开 出愿段的哭咒的天蛦丘好落 拜托当血穿永杨一定的诗篇 我给你的爱写在西元前 深埋在美索不达米亚平原 </span><br><span class="line"> - 不分开扫把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 我来了我 在我感外的溪边河口默默</span><br></pre></td></tr></table></figure>

<p>接下来采用相邻采样训练模型并创作歌词。</p>
<p>In [14]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, False, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line">epoch 50, perplexity 60.294393, time 0.74 sec</span><br><span class="line"> - 分开 我想要你想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我</span><br><span class="line"> - 不分开 我想要你 你有了 别不我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我</span><br><span class="line">epoch 100, perplexity 7.141162, time 0.72 sec</span><br><span class="line"> - 分开 我已要再爱 我不要再想 我不 我不 我不要再想 我不 我不 我不要 爱情我的见快就像龙卷风 离能开</span><br><span class="line"> - 不分开柳 你天黄一个棍 后知哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 </span><br><span class="line">epoch 150, perplexity 2.090277, time 0.73 sec</span><br><span class="line"> - 分开 我已要这是你在著 不想我都做得到 但那个人已经不是我 没有你在 我却多难熬  没有你在我有多难熬多</span><br><span class="line"> - 不分开觉 你已经离 我想再好 这样心中 我一定带我 我的完空 不你是风 一一彩纵 在人心中 我一定带我妈走</span><br><span class="line">epoch 200, perplexity 1.305391, time 0.77 sec</span><br><span class="line"> - 分开 我已要这样牵看你的手 它一定实现它一定像现 载著你 彷彿载著阳光 不管到你留都是晴天 蝴蝶自在飞力</span><br><span class="line"> - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生</span><br><span class="line">epoch 250, perplexity 1.230800, time 0.79 sec</span><br><span class="line"> - 分开 我不要 是你看的太快了悲慢 担心今手身会大早 其么我也睡不着  昨晚梦里你来找 我才  原来我只想</span><br><span class="line"> - 不分开觉 你在经离开我 不知不觉 你知了有节奏 后知后觉 后知了一个秋 后知后觉 我该好好生活 我该好好生</span><br></pre></td></tr></table></figure>


      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/">深度学习Task2</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">Maxwell</a></p>
        <p><span>发布时间:</span>2020-02-14, 21:27:19</p>
        <p><span>最后更新:</span>2020-02-14, 21:39:50</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/" title="深度学习Task2">https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/</a>
            <span class="copy-path" data-clipboard-text="原文: https://chaomeiyan.github.io/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/　　作者: Maxwell" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/9999/02/09/%E7%BD%AE%E9%A1%B6/">
                    置顶
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2020/02/14/hhh/">
                    hhh
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#文本预处理"><span class="toc-number">1.</span> <span class="toc-text">文本预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#读入文本"><span class="toc-number">1.1.</span> <span class="toc-text">读入文本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#分词"><span class="toc-number">1.2.</span> <span class="toc-text">分词</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#建立字典"><span class="toc-number">1.3.</span> <span class="toc-text">建立字典</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#将词转为索引"><span class="toc-number">1.4.</span> <span class="toc-text">将词转为索引</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#用现有工具进行分词"><span class="toc-number">1.5.</span> <span class="toc-text">用现有工具进行分词</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#语言模型"><span class="toc-number">2.</span> <span class="toc-text">语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#定义"><span class="toc-number">2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#计算"><span class="toc-number">2.2.</span> <span class="toc-text">计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#n元语法"><span class="toc-number">2.3.</span> <span class="toc-text">n元语法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#循环神经网络基础"><span class="toc-number">3.</span> <span class="toc-text">循环神经网络基础</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#循环网络的构造"><span class="toc-number">3.1.</span> <span class="toc-text">循环网络的构造</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#从零开始实现循环神经网络"><span class="toc-number">3.2.</span> <span class="toc-text">从零开始实现循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#one-hot向量"><span class="toc-number">3.2.1.</span> <span class="toc-text">one-hot向量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#初始化模型参数"><span class="toc-number">3.2.2.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#定义模型"><span class="toc-number">3.2.3.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#裁剪梯度"><span class="toc-number">3.2.4.</span> <span class="toc-text">裁剪梯度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#定义预测函数"><span class="toc-number">3.2.5.</span> <span class="toc-text">定义预测函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#困惑度"><span class="toc-number">3.2.6.</span> <span class="toc-text">困惑度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#定义模型训练函数"><span class="toc-number">3.2.7.</span> <span class="toc-text">定义模型训练函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#训练模型并创作歌词"><span class="toc-number">3.2.8.</span> <span class="toc-text">训练模型并创作歌词</span></a></li></ol></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    
        <div class="bdsharebuttonbox">
            <a href="#" class="fa fa-twitter bds_twi" data-cmd="twi" title="分享到推特"></a>
            <a href="#" class="fa fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
            <a href="#" class="fa fa-qq bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
            <a href="#" class="fa fa-files-o bds_copy" data-cmd="copy" title="复制网址"></a>
            <a href="#" class="fa fa fa-envelope-o bds_mail" data-cmd="mail" title="通过邮件分享"></a>
            <a href="#" class="fa fa-weixin bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
            <a href="#" class="fa fa-share-alt bds_more" data-cmd="more"></i></a>
        </div>
        <script>
            window._bd_share_config={
                "common":{"bdSnsKey":{},"bdText":"深度学习Task2　| Maxwell's Blog　","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
        </script>
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/9999/02/09/%E7%BD%AE%E9%A1%B6/" title="上一篇: 置顶">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2020/02/14/hhh/" title="下一篇: hhh">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/9999/02/09/%E7%BD%AE%E9%A1%B6/">置顶</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/">深度学习Task2</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/14/hhh/">hhh</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task1/">深度学习Task1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/14/python%E4%B8%ADyield%E7%9A%84%E7%94%A8%E6%B3%95%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94%E6%9C%80%E7%AE%80%E5%8D%95%EF%BC%8C%E6%9C%80%E6%B8%85%E6%99%B0%E7%9A%84%E8%A7%A3%E9%87%8A/">python中yield的用法详解——最简单，最清晰的解释</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/14/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/">Python数据结构和算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/13/markdown%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">markdown学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/12/%E9%80%92%E5%BD%92%E6%96%B9%E6%B3%95%E8%AE%BA/">递归方法论</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/11/%E4%B8%93%E6%B3%A8%E5%8A%9B%E4%B8%8E%E6%89%A7%E8%A1%8C%E5%8A%9B%E6%98%AF%E4%BA%BA%E4%B8%8E%E4%BA%BA%E4%B9%8B%E9%97%B4%E4%BA%A7%E7%94%9F%E5%B7%AE%E5%88%AB%E7%9A%84%E4%B8%BB%E8%A6%81%E5%8E%9F%E5%9B%A0/">专注力与执行力是人与人之间产生差别的主要原因</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/09/ACM%E5%85%A5%E9%97%A8/">ACM入门</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2020 Maxwell
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
            
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>