<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Maxwell" />


    
    


<meta name="description" content="中国科学技术大学计算机学院大二在读">
<meta property="og:type" content="website">
<meta property="og:title" content="Maxwell&#39;s Blog">
<meta property="og:url" content="https://chaomeiyan.github.io/.com/page/2/index.html">
<meta property="og:site_name" content="Maxwell&#39;s Blog">
<meta property="og:description" content="中国科学技术大学计算机学院大二在读">
<meta property="article:author" content="Maxwell">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Maxwell&#39;s Blog" type="application/atom+xml">



    <link rel="shortcut icon" href="/apple-touch-icon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Maxwell&#39;s Blog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/me.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Maxwell</a></h1>
        </hgroup>

        
        <p class="header-subtitle">中国科学技术大学计算机学院大二在读</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="https://mail.ustc.edu.cn/" target="_blank" rel="noopener" title="Email"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" href="https://www.zhihu.com/people/yan-chao-mei-53" target="_blank" rel="noopener" title="知乎"></a>
                            
                                <a class="fa CSDN" href="https://mp.csdn.net/" target="_blank" rel="noopener" title="CSDN"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ACM/" rel="tag">ACM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C%E8%AF%AD%E8%A8%80/" rel="tag">C语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pyhon/" rel="tag">Pyhon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/" rel="tag">基础算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7/" rel="tag">学习技巧</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%85%B7/" rel="tag">工具</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%84%9F%E6%83%B3/" rel="tag">感想</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/" rel="tag">程序设计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" rel="tag">编程语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BD%AE%E9%A1%B6/" rel="tag">置顶</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io" target="_blank" rel="noopener">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/" target="_blank" rel="noopener">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/" target="_blank" rel="noopener">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">中国科学技术大学计算机学院大二在读</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Maxwell</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/me.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Maxwell</a></h1>
            </hgroup>
            
            <p class="header-subtitle">中国科学技术大学计算机学院大二在读</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="https://mail.ustc.edu.cn/" title="Email"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa 知乎" target="_blank" href="https://www.zhihu.com/people/yan-chao-mei-53" title="知乎"></a>
                            
                                <a class="fa CSDN" target="_blank" href="https://mp.csdn.net/" title="CSDN"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-C语言缓冲区详解" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/20/C%E8%AF%AD%E8%A8%80%E7%BC%93%E5%86%B2%E5%8C%BA%E8%AF%A6%E8%A7%A3/" class="article-date">
      <time datetime="2020-02-20T05:36:45.000Z" itemprop="datePublished">2020-02-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/20/C%E8%AF%AD%E8%A8%80%E7%BC%93%E5%86%B2%E5%8C%BA%E8%AF%A6%E8%A7%A3/">C语言缓冲区详解</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="c语言-缓冲区详解"><a href="#c语言-缓冲区详解" class="headerlink" title="c语言 - 缓冲区详解"></a>c语言 - 缓冲区详解</h2><p>c语言 - 缓冲区详解，<code>缓冲区</code>(Buffer)又称为<code>缓存</code>(Cache)，是内存空间的一部分<br>也就是说，在内存中预留了一定的存储空间，用来暂时保存输入或输出的数据，这部分预留的空间就叫做缓冲区</p>
<h2 id="缓冲区"><a href="#缓冲区" class="headerlink" title="缓冲区"></a>缓冲区</h2><p>缓冲区根据其对应的是<code>输入设备</code>还是<code>输出设备</code>，分为<code>输入缓冲区</code>和<code>输出缓冲区</code></p>
<p><strong>为什么要引入缓冲区</strong>？<br>比如从磁盘里取信息，我们先把读出的数据放在缓冲区，计算机再直接从缓冲区中取数据，等缓冲区的数据取完后再去磁盘中读取，这样就可以减少磁盘的读写次数，再加上计算机对缓冲区的操作大大快于对磁盘的操作，故应用缓冲区可大大提高计算机的运行速度。</p>
<p>又比如，我们使用打印机打印文档，由于打印机的打印速度相对较慢，我们先把文档输出到打印机相应的缓冲区，打印机再自行逐步打印，这时我们的CPU可以处理别的事情。</p>
<p>现在你基本明白了吧，缓冲区就是一块内存区，它用在输入输出设备和CPU之间，用来缓存数据。它使得低速的输入输出设备和高速的CPU能够协调工作，避免低速的输入输出设备占用CPU，解放出CPU，使其能够高效率工作。</p>
<h2 id="缓冲区类型"><a href="#缓冲区类型" class="headerlink" title="缓冲区类型"></a>缓冲区类型</h2><p>缓冲区分为三种类型：全缓冲、行缓冲和不带缓冲</p>
<ul>
<li><code>全缓冲</code><br>在这种情况下，当填满缓冲区后才进行实际I/O操作。<br>全缓冲的典型代表是对磁盘文件的读写。</li>
<li><code>行缓冲</code><br>在这种情况下，当在输入和输出中遇到换行符时，执行真正的I/O操作。<br>这时，我们输入的字符先存放在缓冲区，等按下回车键换行时才进行实际的I/O操作。<br>典型代表是标准输入(stdin)和标准输出(stdout)。</li>
<li><code>不带缓冲</code><br>也就是不进行缓冲，标准错误文件 stderr 是典型代表，这使得出错信息可以直接尽快地显示出来</li>
</ul>
<h2 id="缓冲区大小"><a href="#缓冲区大小" class="headerlink" title="缓冲区大小"></a>缓冲区大小</h2><p>如果我们没有自己设置缓冲区的话，系统会默认为标准输入输出设置一个缓冲区，这个缓冲区的大小通常是<code>512个字节</code>的大小</p>
<p>缓冲区大小由 stdio.h 头文件中的宏 BUFSIZ 定义，如果希望查看它的大小，包含头文件，直接输出它的值即可<br>缓冲区的大小是可以改变的，也可以将文件关联到自定义的缓冲区，详情可以查看 <code>setvbuf()</code> 和 <code>setbuf()</code> 函数</p>
<h2 id="刷新缓冲区"><a href="#刷新缓冲区" class="headerlink" title="刷新缓冲区"></a>刷新缓冲区</h2><p>下列情况会引发缓冲区的刷新：</p>
<ul>
<li>缓冲区满时</li>
<li>行缓冲区遇到回车时</li>
<li>关闭文件</li>
<li>使用特定函数刷新缓冲区</li>
</ul>
<p><strong>scanf() 缓冲区的影响</strong><br>scanf()是带有缓冲区的，scanf()先从缓冲区获取数据，如果缓冲区没有数据，则等待用户输入<br>scanf()匹配到想要的数据之后，就会将匹配到的数据从缓冲区中删除，没有匹配到的数据不被影响</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a, b;</span><br><span class="line">    <span class="keyword">char</span> str[<span class="number">50</span>];</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"b: "</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;b);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;a);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;b);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%s"</span>, str);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"a: %d, b: %d, str: %s\n"</span>, a, b, str);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># root @ localhost in ~ [18:51:11]</span></span><br><span class="line">$ gcc a.c</span><br><span class="line"></span><br><span class="line"><span class="meta"># root @ localhost in ~ [18:51:12]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">b: <span class="number">99</span></span><br><span class="line"><span class="number">11</span> <span class="number">22</span> <span class="built_in">string</span></span><br><span class="line">a: <span class="number">11</span>, b: <span class="number">22</span>, str: <span class="built_in">string</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># root @ localhost in ~ [18:51:25]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">b: <span class="number">99</span></span><br><span class="line"><span class="number">11</span> <span class="built_in">string</span></span><br><span class="line">a: <span class="number">11</span>, b: <span class="number">99</span>, str: <span class="built_in">string</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># root @ localhost in ~ [18:51:35]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">b: <span class="number">99</span></span><br><span class="line"><span class="built_in">string</span></span><br><span class="line">a: <span class="number">0</span>, b: <span class="number">99</span>, str: <span class="built_in">string</span></span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<p>需要注意的是，从命令行输入的都是字符串类型，都可以被<code>%s</code>匹配</p>
<p>输入缓冲区虽然方便，但是有时候却会带来别的麻烦：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m, n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"m=%d"</span>, &amp;m);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"n=%d"</span>, &amp;n);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"m=%d, n=%d\n"</span>, m, n);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># root @ localhost in ~ [19:01:19]</span></span><br><span class="line">$ gcc a.c</span><br><span class="line"></span><br><span class="line"><span class="meta"># root @ localhost in ~ [19:01:49]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">m=<span class="number">10</span></span><br><span class="line">m=<span class="number">10</span>, n=<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<p>我们还没有输入n，就直接打印数值了，这就是输入缓冲区导致的。输入<code>m=10</code>然后按下回车，第一个scanf()匹配成功，这时候缓冲区的内容是换行符，一般情况下scanf()会忽略它，但是当控制字符串不是以<code>%xxx</code>开头时，就不会忽略了，比如，下面这种情况就会被scanf()忽略。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m, n;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"enter 2 nums:\n"</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;m);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"m = %d, n = %d\n"</span>, m, n);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># root @ localhost in ~ [19:07:31]</span></span><br><span class="line">$ ./a.out</span><br><span class="line">enter <span class="number">2</span> nums:</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line">m = <span class="number">10</span>, n = <span class="number">20</span></span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
<p><strong>清空缓冲区</strong><br>清空缓冲区主要有两种思路：一是将缓冲区的数据直接丢弃，二是将缓冲区的数据读取出来，但不使用<br>第一种思路：<br>windows下用<code>fflush(stdin)</code>，linux下用<code>setbuf(stdin, NULL)</code>，这两个函数都在头文件<code>stdio.h</code>中</p>
<p>第二种思路：</p>
<ul>
<li><p>第一种方法</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> c;</span><br><span class="line"><span class="keyword">while</span>((c = getchar()) != <span class="string">'\n'</span> &amp;&amp; c != EOF);</span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
</li>
<li><p>第二种方法</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">scanf</span>(<span class="string">"%*[^\n]%*c"</span>);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">%*[^\n] 匹配除了换行符之外的字符，前面的*号表示不保存这些数据，直接丢弃</span></span><br><span class="line"><span class="comment">%c      匹配剩下的换行符，*号的作用同上</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<p>C</p>
<p>Copy</p>
</li>
</ul>
<p>综上所述，如果不考虑移植性，就用第一种思路的方法，如果考虑移植性，建议用第二种思路的第二种方法</p>
<p><strong>getch() getche() getchar()</strong><br>getchar()是有缓冲区的，而getche()和getch()没有缓冲区</p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/C%E8%AF%AD%E8%A8%80/" rel="tag">C语言</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-数据结构——堆" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%A0%86/" class="article-date">
      <time datetime="2020-02-15T08:11:03.000Z" itemprop="datePublished">2020-02-15</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E5%A0%86/">数据结构——堆</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="Python中的堆：heapq"><a href="#Python中的堆：heapq" class="headerlink" title="Python中的堆：heapq"></a>Python中的堆：heapq</h3><p><strong>1、heappush(heap,n)数据堆入</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> heapq <span class="keyword">as</span> hq</span><br><span class="line">In [<span class="number">2</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">In [<span class="number">3</span>]: data = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#将生成的数据随机打乱顺序</span></span><br><span class="line">In [<span class="number">4</span>]: np.random.shuffle(data)</span><br><span class="line">In [<span class="number">5</span>]: data</span><br><span class="line">Out[<span class="number">5</span>]: array([<span class="number">5</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"><span class="comment">#定义heap列表</span></span><br><span class="line">In [<span class="number">6</span>]: heap = []</span><br><span class="line"><span class="comment">#使用heapq库的heappush函数将数据堆入</span></span><br><span class="line">In [<span class="number">7</span>]: <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">   ...:     hq.heappush(heap,i)</span><br><span class="line">   ...:</span><br><span class="line">In [<span class="number">8</span>]: heap</span><br><span class="line">Out[<span class="number">8</span>]: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: hq.heappush(heap,<span class="number">0.5</span>)</span><br><span class="line">In [<span class="number">10</span>]: heap</span><br><span class="line">Out[<span class="number">10</span>]: [<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<p><strong>2、heappop(heap)将数组堆中的最小元素弹出</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: hq.heappop(heap)</span><br><span class="line">Out[<span class="number">11</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: hq.heappop(heap)</span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">0.5</span></span><br></pre></td></tr></table></figure>

<p><strong>3、heapify(heap) 将heap属性强制应用到任意一个列表</strong></p>
<p>heapify 函数将使用任意列表作为参数，并且尽可能少的移位操作，，将其转化为合法的堆。如果没有建立堆，那么在使用heappush和heappop前应该使用该函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: heap = [<span class="number">5</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: hq.heapify(heap)</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: heap</span><br><span class="line">Out[<span class="number">15</span>]: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<p><strong>4、heapreplace(heap，n)弹出最小的元素被n替代</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: hq.heapreplace(heap,<span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">17</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: heap</span><br><span class="line">Out[<span class="number">18</span>]: [<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure>

<p><strong>5、nlargest(n,iter)、nsmallest(n,iter)</strong><br>heapq中剩下的两个函数nlargest(n.iter)和nsmallest(n.iter)分别用来寻找任何可迭代的对象iter中第n大或者第n小的元素。可以通过使用排序（sorted函数）和分片进行完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回第一个最大的数</span></span><br><span class="line">In [<span class="number">19</span>]: hq.nlargest(<span class="number">1</span>,heap)</span><br><span class="line">Out[<span class="number">19</span>]: [<span class="number">9</span>]</span><br><span class="line"><span class="comment">#返回第一个最小的数</span></span><br><span class="line">In [<span class="number">20</span>]: hq.nsmallest(<span class="number">1</span>,heap)</span><br><span class="line">Out[<span class="number">20</span>]: [<span class="number">0.5</span>]</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-拿到高GPA技巧——持续更新" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/14/%E6%8B%BF%E5%88%B0%E9%AB%98GPA%E6%8A%80%E5%B7%A7%E2%80%94%E2%80%94%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/" class="article-date">
      <time datetime="2020-02-14T15:23:07.000Z" itemprop="datePublished">2020-02-14</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/14/%E6%8B%BF%E5%88%B0%E9%AB%98GPA%E6%8A%80%E5%B7%A7%E2%80%94%E2%80%94%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/">拿到高GPA技巧——持续更新</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="01建立一个目标"><a href="#01建立一个目标" class="headerlink" title="01建立一个目标"></a>01建立一个目标</h3><p>首先要建立一个具体的目标，比如这学期绩点达到3.8以上。目标要适合自己的水平，比自己上学期绩点高5%~10%左右比较合适。</p>
<p>然后就是要<strong>细化目标</strong></p>
<p>没有人能够一步登天，仅仅靠期末是<strong>极其容易</strong>翻车的</p>
<p>可以分阶段设置目标，设置目标的时候需要将每门课分拆，细化得分点，比如一门专业课大概分为<strong>平时分（点名，作业，PRE等等）+期中（可能没有）+期末+实验</strong>， 需要明确的是每一部分都需要认认真真对待，因为你不知道老师会不会为了调分而大幅度修改各项的比例。原则就是要有每一分都要拿到手的心态。 </p>
<p>​        尽量知道每一项的评分细则，特别是实验，在知道评分细则之后就去拟合就行，尽量拿分。</p>
<p>​        然后为了达成期中和期末的好分数，需要给自己每周设定一个小任务，当完成了一项小任务，会对自己的实力有更加清晰的认知，这样也能带动接下来任务的成功。</p>
<p><strong>分析目标最优解</strong>：</p>
<p>​        最优解就是要在给定的时间内达到最大的效益，也就是要确定任务的优先级，这是很必要的，当有很多事情向做的时候需要给他们排个优先级，达到最大效益，一般来说要把难的放在比较前面做，一股气解决。</p>
<h4 id="01-1平时分拿分诀窍"><a href="#01-1平时分拿分诀窍" class="headerlink" title="01.1平时分拿分诀窍"></a>01.1平时分拿分诀窍</h4><p>平时分包括作业+课堂表现+小组活动+老师印象分</p>
<p>作业是以周为单位的，需要每周做好预习，认真听课，认真复习，然后再认认真真完成作业。最好和同学对一下答案再交，保证作业的准确率。不过切记<strong>一定要自己认认真真做过思考过</strong>，在每次上课时要跟着老师的思路，坚决不玩手机，敢于回答老师的问题，小组合作时积极参加，踊跃地当发言人。在课后要多去问一些有价值的问题以及可以指出老师讲课的某个小错误。<strong>不管什么课尽量去第三排，一堂课对老师紧紧跟随，老师提问问题时尽量提出自己的想法，</strong> </p>
<p><strong>预习+独立完成作业+复盘</strong>，这是每周的一个小循环，每天晚上用25min预习第二天的科目，勾画重点和不懂的地方，最大用处是<strong>防止上课走神就听不懂了</strong>     独立完成作业最重要的就是<strong>独立</strong>，一道题从<strong>审题思考做题再回过头反思</strong>最离不开的就是<strong>独立</strong> 独立做作业就是<strong>把知识内化的过程</strong> 如果早早得翻看答案或者与人交流那自己不过是个计算器。     最后是<strong>复盘</strong>，复盘是<strong>复习上课的笔记，书上的重点，还有作业和以前考试的错题</strong>，以及思考最近的知识掌握情况，这些知识之间的关联，<strong>每周结束时画个思维导图，构建自己的知识树</strong></p>
<h4 id="01-2期中和期末考试复习技巧"><a href="#01-2期中和期末考试复习技巧" class="headerlink" title="01.2期中和期末考试复习技巧"></a>01.2期中和期末考试复习技巧</h4><p>首先要明确一个观点：<strong>除了特别硬核的课之外，大部分课只靠期末冲刺拿到85分左右的成绩是不难的，只要划分20~40个小时的时间去有步骤地复习</strong></p>
<p>切记：<strong>一定要把复习时间精确到小时</strong></p>
<p>很多人考试周也会抱着电脑和书去图书馆坐一天，但可能大部分时间被其他一些无关紧要的东西给中断了，比如看B站，看知乎，看QQ，微信，小说等等，这种行为的本质原因是专注力不够，是一定要克服的，不然谈什么技巧都是白搭。一天结束看似一直在图书馆，但学习时间可能总共都不够一个小时，这就血妈亏，三天下来还不如别人一个晚上，这就叫<strong>虚假努力，自我感动</strong>，你想想怎么可能拿高分嘛。所以需要实实在在得<strong>根据期末周的安排把每门课的复习量精确到小时并且坚决地执行</strong>，推荐<strong>番茄todo</strong>， 以周为单位去规划平时要做的事情，<strong>对任务量的估计要做到把自己估计需要的时间乘个1.5，多试几次就能掌握自己需要的任务时间。</strong></p>
<h3 id="02及时复盘"><a href="#02及时复盘" class="headerlink" title="02及时复盘"></a>02及时复盘</h3><p>每一天，每一周，每一个阶段都要去做一个反思，看自己的学习状态怎么样，有没有达到自己的目标，没有的话要怎么去解决，对自己的计划不断地做修正。</p>
<p>其实这些技巧都建立在一个事实上——你足够专注，能够在一个小时以上投入地做一件事情，以及坚持每天都做，嗯，<strong>学不会不要紧，再来就行了</strong>，重要的是要有重复再来的坚持与相信自己能学会的信心，<strong>当做一个任务没那么舒服没那么容易的时候要去享受，以为你在提升啊，这多爽</strong>，每天给自己的评价就是<strong>做了什么事情，学了多少东西，</strong></p>
<p>其实以上最重要的还是<strong>坚持</strong>，以及保持足够的<strong>专注</strong></p>
<h3 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h3><p>除了以上所说的那些，还想补充一些观点。就是在学习过程中要注意两个点，就是<strong>监督信息</strong>和<strong>反馈</strong> 监督是说你看书或者听课或者看文章等等是不够的，你还需要去做习题，去写代码，去复现一波，再去思考蕴含的东西，才能真正的学会学好，而这带来的成就感也会作为一个正反馈让自己爱上这种感觉，从而达到<strong>持续专注</strong> 以及<strong>提高自己的执行力。</strong> </p>
<p>也就是说  <strong>只看书或者听课而不刷题写代码是学不会东西的，反而会造成我已经学会了的假象</strong>（就像我的模电一样） 还有就是这里的刷题写代码一定得是独立自主的，不能直接抄答案，不然后悔的一定是自己</p>
<p>“现在我认为真正的学习应该是<strong>非常“主动”地去进行</strong>，看书、论文的时候要对框架和motivation有一个清晰的把握，知道/理解算法这样设计的目的是什么，并要相对频繁地去ask some questions，批判地进行阅读与消化。但可惜当时的自己还差得很远。另一方面，自己习题、coding做得太少，没有获得足够的<strong>监督信息</strong>，自以为自己学懂了，但其实只学了点皮毛。好在那时候还算是通过一些课程的作业获得了很多<strong>正反馈</strong>，支持着我的学习热情。”</p>
<p>对此我也有体会，就是要主动去掌握这门课的一个大体框架，然后再一章一章去突破 而且要主动的思考，主动刷题，主动总结，<strong>多对自己提问</strong>，</p>
<p>​        </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E6%8A%80%E5%B7%A7/" rel="tag">学习技巧</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Python字符串和文本" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/14/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%96%87%E6%9C%AC/" class="article-date">
      <time datetime="2020-02-14T15:11:09.000Z" itemprop="datePublished">2020-02-14</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/14/Python%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%96%87%E6%9C%AC/">Python字符串和文本</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pyhon/" rel="tag">Pyhon</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" rel="tag">编程语言</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-深度学习Task2" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/" class="article-date">
      <time datetime="2020-02-14T13:27:19.000Z" itemprop="datePublished">2020-02-14</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0Task2/">深度学习Task2</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h3><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<ol>
<li>读入文本</li>
<li>分词</li>
<li>建立字典，将每个词映射到一个唯一的索引（index）</li>
<li>将文本从词的序列转换为索引的序列，方便输入模型</li>
</ol>
<h4 id="读入文本"><a href="#读入文本" class="headerlink" title="读入文本"></a>读入文本</h4><p>我们用一部英文小说，即H. G. Well的<a href="http://www.gutenberg.org/ebooks/35" target="_blank" rel="noopener">Time Machine</a>，作为示例，展示文本预处理的具体过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import collections</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">def read_time_machine():</span><br><span class="line">    with open(&#39;&#x2F;home&#x2F;kesci&#x2F;input&#x2F;timemachine7163&#x2F;timemachine.txt&#39;, &#39;r&#39;) as f:</span><br><span class="line">        lines &#x3D; [re.sub(&#39;[^a-z]+&#39;, &#39; &#39;, line.strip().lower()) for line in f]</span><br><span class="line">    return lines</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lines &#x3D; read_time_machine()</span><br><span class="line">print(&#39;# sentences %d&#39; % len(lines))</span><br><span class="line"># sentences 3221</span><br></pre></td></tr></table></figure>

<h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def tokenize(sentences, token&#x3D;&#39;word&#39;):</span><br><span class="line">    &quot;&quot;&quot;Split sentences into word or char tokens&quot;&quot;&quot;</span><br><span class="line">    if token &#x3D;&#x3D; &#39;word&#39;:</span><br><span class="line">        return [sentence.split(&#39; &#39;) for sentence in sentences]</span><br><span class="line">    elif token &#x3D;&#x3D; &#39;char&#39;:</span><br><span class="line">        return [list(sentence) for sentence in sentences]</span><br><span class="line">    else:</span><br><span class="line">        print(&#39;ERROR: unkown token type &#39;+token)</span><br><span class="line"></span><br><span class="line">tokens &#x3D; tokenize(lines)</span><br><span class="line">tokens[0:2]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[&#39;the&#39;, &#39;time&#39;, &#39;machine&#39;, &#39;by&#39;, &#39;h&#39;, &#39;g&#39;, &#39;wells&#39;, &#39;&#39;], [&#39;&#39;]]</span><br></pre></td></tr></table></figure>

<h4 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h4><p>为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class Vocab(object):</span><br><span class="line">    def __init__(self, tokens, min_freq&#x3D;0, use_special_tokens&#x3D;False):</span><br><span class="line">        counter &#x3D; count_corpus(tokens)  # : </span><br><span class="line">        self.token_freqs &#x3D; list(counter.items())</span><br><span class="line">        self.idx_to_token &#x3D; []</span><br><span class="line">        if use_special_tokens:</span><br><span class="line">            # padding, begin of sentence, end of sentence, unknown</span><br><span class="line">            self.pad, self.bos, self.eos, self.unk &#x3D; (0, 1, 2, 3)</span><br><span class="line">            self.idx_to_token +&#x3D; [&#39;&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;]</span><br><span class="line">        else:</span><br><span class="line">            self.unk &#x3D; 0</span><br><span class="line">            self.idx_to_token +&#x3D; [&#39;&#39;]</span><br><span class="line">        self.idx_to_token +&#x3D; [token for token, freq in self.token_freqs</span><br><span class="line">                        if freq &gt;&#x3D; min_freq and token not in self.idx_to_token]</span><br><span class="line">        self.token_to_idx &#x3D; dict()</span><br><span class="line">        for idx, token in enumerate(self.idx_to_token):</span><br><span class="line">            self.token_to_idx[token] &#x3D; idx</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, tokens):</span><br><span class="line">        if not isinstance(tokens, (list, tuple)):</span><br><span class="line">            return self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        return [self.__getitem__(token) for token in tokens]</span><br><span class="line"></span><br><span class="line">    def to_tokens(self, indices):</span><br><span class="line">        if not isinstance(indices, (list, tuple)):</span><br><span class="line">            return self.idx_to_token[indices]</span><br><span class="line">        return [self.idx_to_token[index] for index in indices]</span><br><span class="line"></span><br><span class="line">def count_corpus(sentences):</span><br><span class="line">    tokens &#x3D; [tk for st in sentences for tk in st]</span><br><span class="line">    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数</span><br></pre></td></tr></table></figure>

<p>我们看一个例子，这里我们尝试用Time Machine作为语料构建字典</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab &#x3D; Vocab(tokens)</span><br><span class="line">print(list(vocab.token_to_idx.items())[0:10])</span><br><span class="line">[(&#39;&#39;, 0), (&#39;the&#39;, 1), (&#39;time&#39;, 2), (&#39;machine&#39;, 3), (&#39;by&#39;, 4), (&#39;h&#39;, 5), (&#39;g&#39;, 6), (&#39;wells&#39;, 7), (&#39;i&#39;, 8), (&#39;traveller&#39;, 9)]</span><br></pre></td></tr></table></figure>

<h4 id="将词转为索引"><a href="#将词转为索引" class="headerlink" title="将词转为索引"></a>将词转为索引</h4><p>使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for i in range(8, 10):</span><br><span class="line">    print(&#39;words:&#39;, tokens[i])</span><br><span class="line">    print(&#39;indices:&#39;, vocab[tokens[i]])</span><br><span class="line">words: [&#39;the&#39;, &#39;time&#39;, &#39;traveller&#39;, &#39;for&#39;, &#39;so&#39;, &#39;it&#39;, &#39;will&#39;, &#39;be&#39;, &#39;convenient&#39;, &#39;to&#39;, &#39;speak&#39;, &#39;of&#39;, &#39;him&#39;, &#39;&#39;]</span><br><span class="line">indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]</span><br><span class="line">words: [&#39;was&#39;, &#39;expounding&#39;, &#39;a&#39;, &#39;recondite&#39;, &#39;matter&#39;, &#39;to&#39;, &#39;us&#39;, &#39;his&#39;, &#39;grey&#39;, &#39;eyes&#39;, &#39;shone&#39;, &#39;and&#39;]</span><br><span class="line">indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]</span><br></pre></td></tr></table></figure>

<h4 id="用现有工具进行分词"><a href="#用现有工具进行分词" class="headerlink" title="用现有工具进行分词"></a>用现有工具进行分词</h4><p>我们前面介绍的分词方式非常简单，它至少有以下几个缺点:</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<p>下面是一个简单的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text &#x3D; &quot;Mr. Chen doesn&#39;t agree with my suggestion.&quot;</span><br></pre></td></tr></table></figure>

<p>spaCy:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import spacy</span><br><span class="line">nlp &#x3D; spacy.load(&#39;en_core_web_sm&#39;)</span><br><span class="line">doc &#x3D; nlp(text)</span><br><span class="line">print([token.text for token in doc])</span><br><span class="line">[&#39;Mr.&#39;, &#39;Chen&#39;, &#39;does&#39;, &quot;n&#39;t&quot;, &#39;agree&#39;, &#39;with&#39;, &#39;my&#39;, &#39;suggestion&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure>

<p>NLTK:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">from nltk import data</span><br><span class="line">data.path.append(&#39;&#x2F;home&#x2F;kesci&#x2F;input&#x2F;nltk_data3784&#x2F;nltk_data&#39;)</span><br><span class="line">print(word_tokenize(text))</span><br><span class="line">[&#39;Mr.&#39;, &#39;Chen&#39;, &#39;does&#39;, &quot;n&#39;t&quot;, &#39;agree&#39;, &#39;with&#39;, &#39;my&#39;, &#39;suggestion&#39;, &#39;.&#39;]</span><br></pre></td></tr></table></figure>

<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为<em>T</em>的文本中的词依次为<em>w</em>1,<em>w</em>2,…,<em>w*</em>T<em>，那么在离散的时间序列中，</em>w<strong>t<em>（1≤</em>t<em>≤</em>T<em>）可看作在时间步（time step）</em>t<em>的输出或标签。给定一个长度为</em>T<em>的词的序列</em>w<em>1,</em>w<em>2,…,</em>w</strong>T*，语言模型将计算该序列的概率：</p>
<p><em>P</em>(<em>w</em>1,<em>w</em>2,…,<em>w**T</em>).</p>
<p>语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。</p>
<h4 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h4><p><img src="http://q5p38kfbx.bkt.clouddn.com/202002142255_493.png" alt></p>
<h4 id="n元语法"><a href="#n元语法" class="headerlink" title="n元语法"></a>n元语法</h4><p><img src="http://q5p38kfbx.bkt.clouddn.com/202002142255_489.png" alt></p>
<h3 id="循环神经网络基础"><a href="#循环神经网络基础" class="headerlink" title="循环神经网络基础"></a>循环神经网络基础</h3><p>上一节介绍的nn元语法中，时间步tt的词wtwt基于前面所有词的条件概率只考虑了最近时间步的n−1n−1个词。如果要考虑比t−(n−1)t−(n−1)更早时间步的词对wtwt的可能影响，我们需要增大nn。但这样模型参数的数量将随之呈指数级增长（可参考上一节的练习）。</p>
<p>本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p>
<h4 id="循环网络的构造"><a href="#循环网络的构造" class="headerlink" title="循环网络的构造"></a>循环网络的构造</h4><p><img src="http://q5p38kfbx.bkt.clouddn.com/202002142256_508.png" alt></p>
<h4 id="从零开始实现循环神经网络"><a href="#从零开始实现循环神经网络" class="headerlink" title="从零开始实现循环神经网络"></a>从零开始实现循环神经网络</h4><p>我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据：</p>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import time</span><br><span class="line">import math</span><br><span class="line">import sys</span><br><span class="line">sys.path.append(&quot;&#x2F;home&#x2F;kesci&#x2F;input&quot;)</span><br><span class="line">import d2l_jay9460 as d2l</span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) &#x3D; d2l.load_data_jay_lyrics()</span><br><span class="line">device &#x3D; torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</span><br></pre></td></tr></table></figure>

<h5 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h5><p>我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是NN，每次字符对应一个从00到N−1N−1的唯一的索引，则该字符的向量是一个长度为NN的向量，若字符的索引是ii，则该向量的第ii个位置为11，其他位置为00。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>
<p>In [2]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def one_hot(x, n_class, dtype&#x3D;torch.float32):</span><br><span class="line">    result &#x3D; torch.zeros(x.shape[0], n_class, dtype&#x3D;dtype, device&#x3D;x.device)  # shape: (n, n_class)</span><br><span class="line">    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] &#x3D; 1</span><br><span class="line">    return result</span><br><span class="line">    </span><br><span class="line">x &#x3D; torch.tensor([0, 2])</span><br><span class="line">x_one_hot &#x3D; one_hot(x, vocab_size)</span><br><span class="line">print(x_one_hot)</span><br><span class="line">print(x_one_hot.shape)</span><br><span class="line">print(x_one_hot.sum(axis&#x3D;1))</span><br><span class="line">tensor([[1., 0., 0.,  ..., 0., 0., 0.],</span><br><span class="line">        [0., 0., 1.,  ..., 0., 0., 0.]])</span><br><span class="line">torch.Size([2, 1027])</span><br><span class="line">tensor([1., 1.])</span><br></pre></td></tr></table></figure>

<p>我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步tt的输入为Xt∈Rn×dXt∈Rn×d，其中nn为批量大小，dd为词向量大小，即one-hot向量长度（词典大小）。</p>
<p>In [3]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def to_onehot(X, n_class):</span><br><span class="line">    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]</span><br><span class="line"></span><br><span class="line">X &#x3D; torch.arange(10).view(2, 5)</span><br><span class="line">inputs &#x3D; to_onehot(X, vocab_size)</span><br><span class="line">print(len(inputs), inputs[0].shape)</span><br><span class="line">5 torch.Size([2, 1027])</span><br></pre></td></tr></table></figure>

<h5 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h5><p>In [4]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs &#x3D; vocab_size, 256, vocab_size</span><br><span class="line"># num_inputs: d</span><br><span class="line"># num_hiddens: h, 隐藏单元的个数是超参数</span><br><span class="line"># num_outputs: q</span><br><span class="line"></span><br><span class="line">def get_params():</span><br><span class="line">    def _one(shape):</span><br><span class="line">        param &#x3D; torch.zeros(shape, device&#x3D;device, dtype&#x3D;torch.float32)</span><br><span class="line">        nn.init.normal_(param, 0, 0.01)</span><br><span class="line">        return torch.nn.Parameter(param)</span><br><span class="line"></span><br><span class="line">    # 隐藏层参数</span><br><span class="line">    W_xh &#x3D; _one((num_inputs, num_hiddens))</span><br><span class="line">    W_hh &#x3D; _one((num_hiddens, num_hiddens))</span><br><span class="line">    b_h &#x3D; torch.nn.Parameter(torch.zeros(num_hiddens, device&#x3D;device))</span><br><span class="line">    # 输出层参数</span><br><span class="line">    W_hq &#x3D; _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q &#x3D; torch.nn.Parameter(torch.zeros(num_outputs, device&#x3D;device))</span><br><span class="line">    return (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>

<h5 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h5><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>
<p>In [5]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rnn(inputs, state, params):</span><br><span class="line">    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q &#x3D; params</span><br><span class="line">    H, &#x3D; state</span><br><span class="line">    outputs &#x3D; []</span><br><span class="line">    for X in inputs:</span><br><span class="line">        H &#x3D; torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        Y &#x3D; torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return outputs, (H,)</span><br></pre></td></tr></table></figure>

<p>函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。</p>
<p>In [6]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def init_rnn_state(batch_size, num_hiddens, device):</span><br><span class="line">    return (torch.zeros((batch_size, num_hiddens), device&#x3D;device), )</span><br></pre></td></tr></table></figure>

<p>做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。</p>
<p>In [7]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape)</span><br><span class="line">print(num_hiddens)</span><br><span class="line">print(vocab_size)</span><br><span class="line">state &#x3D; init_rnn_state(X.shape[0], num_hiddens, device)</span><br><span class="line">inputs &#x3D; to_onehot(X.to(device), vocab_size)</span><br><span class="line">params &#x3D; get_params()</span><br><span class="line">outputs, state_new &#x3D; rnn(inputs, state, params)</span><br><span class="line">print(len(inputs), inputs[0].shape)</span><br><span class="line">print(len(outputs), outputs[0].shape)</span><br><span class="line">print(len(state), state[0].shape)</span><br><span class="line">print(len(state_new), state_new[0].shape)</span><br><span class="line">torch.Size([2, 5])</span><br><span class="line">256</span><br><span class="line">1027</span><br><span class="line">5 torch.Size([2, 1027])</span><br><span class="line">5 torch.Size([2, 1027])</span><br><span class="line">1 torch.Size([2, 256])</span><br><span class="line">1 torch.Size([2, 256])</span><br></pre></td></tr></table></figure>

<h5 id="裁剪梯度"><a href="#裁剪梯度" class="headerlink" title="裁剪梯度"></a>裁剪梯度</h5><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 gg，并设裁剪的阈值是θθ。裁剪后的梯度</p>
<p>min(θ∥g∥,1)gmin(θ‖g‖,1)g</p>
<p>的L2L2范数不超过θθ。</p>
<p>In [8]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def grad_clipping(params, theta, device):</span><br><span class="line">    norm &#x3D; torch.tensor([0.0], device&#x3D;device)</span><br><span class="line">    for param in params:</span><br><span class="line">        norm +&#x3D; (param.grad.data ** 2).sum()</span><br><span class="line">    norm &#x3D; norm.sqrt().item()</span><br><span class="line">    if norm &gt; theta:</span><br><span class="line">        for param in params:</span><br><span class="line">            param.grad.data *&#x3D; (theta &#x2F; norm)</span><br></pre></td></tr></table></figure>

<h5 id="定义预测函数"><a href="#定义预测函数" class="headerlink" title="定义预测函数"></a>定义预测函数</h5><p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。这个函数稍显复杂，其中我们将循环神经单元<code>rnn</code>设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</p>
<p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,</span><br><span class="line">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):</span><br><span class="line">    state &#x3D; init_rnn_state(1, num_hiddens, device)</span><br><span class="line">    output &#x3D; [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符</span><br><span class="line">    for t in range(num_chars + len(prefix) - 1):</span><br><span class="line">        # 将上一时间步的输出作为当前时间步的输入</span><br><span class="line">        X &#x3D; to_onehot(torch.tensor([[output[-1]]], device&#x3D;device), vocab_size)</span><br><span class="line">        # 计算输出和更新隐藏状态</span><br><span class="line">        (Y, state) &#x3D; rnn(X, state, params)</span><br><span class="line">        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span><br><span class="line">        if t &lt; len(prefix) - 1:</span><br><span class="line">            output.append(char_to_idx[prefix[t + 1]])</span><br><span class="line">        else:</span><br><span class="line">            output.append(Y[0].argmax(dim&#x3D;1).item())</span><br><span class="line">    return &#39;&#39;.join([idx_to_char[i] for i in output])</span><br></pre></td></tr></table></figure>

<p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p>
<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_rnn(&#39;分开&#39;, 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span><br><span class="line">            device, idx_to_char, char_to_idx)</span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#39;分开濡时食提危踢拆田唱母&#39;</span><br></pre></td></tr></table></figure>

<h5 id="困惑度"><a href="#困惑度" class="headerlink" title="困惑度"></a>困惑度</h5><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下<a href="https://staticcdn.boyuai.com/course/jupyter/chapter_deep-learning-basics/softmax-regression.ipynb" target="_blank" rel="noopener">“softmax回归”</a>一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>
<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
<p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小<code>vocab_size</code>。</p>
<h5 id="定义模型训练函数"><a href="#定义模型训练函数" class="headerlink" title="定义模型训练函数"></a>定义模型训练函数</h5><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>
<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li>
</ol>
<p>In [11]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span><br><span class="line">                          lr, clipping_theta, batch_size, pred_period,</span><br><span class="line">                          pred_len, prefixes):</span><br><span class="line">    if is_random_iter:</span><br><span class="line">        data_iter_fn &#x3D; d2l.data_iter_random</span><br><span class="line">    else:</span><br><span class="line">        data_iter_fn &#x3D; d2l.data_iter_consecutive</span><br><span class="line">    params &#x3D; get_params()</span><br><span class="line">    loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态</span><br><span class="line">            state &#x3D; init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">        l_sum, n, start &#x3D; 0.0, 0, time.time()</span><br><span class="line">        data_iter &#x3D; data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class="line">        for X, Y in data_iter:</span><br><span class="line">            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态</span><br><span class="line">                state &#x3D; init_rnn_state(batch_size, num_hiddens, device)</span><br><span class="line">            else:  # 否则需要使用detach函数从计算图分离隐藏状态</span><br><span class="line">                for s in state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">            inputs &#x3D; to_onehot(X, vocab_size)</span><br><span class="line">            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span><br><span class="line">            (outputs, state) &#x3D; rnn(inputs, state, params)</span><br><span class="line">            # 拼接之后形状为(num_steps * batch_size, vocab_size)</span><br><span class="line">            outputs &#x3D; torch.cat(outputs, dim&#x3D;0)</span><br><span class="line">            # Y的形状是(batch_size, num_steps)，转置后再变成形状为</span><br><span class="line">            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span><br><span class="line">            y &#x3D; torch.flatten(Y.T)</span><br><span class="line">            # 使用交叉熵损失计算平均分类误差</span><br><span class="line">            l &#x3D; loss(outputs, y.long())</span><br><span class="line">            </span><br><span class="line">            # 梯度清0</span><br><span class="line">            if params[0].grad is not None:</span><br><span class="line">                for param in params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params, clipping_theta, device)  # 裁剪梯度</span><br><span class="line">            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均</span><br><span class="line">            l_sum +&#x3D; l.item() * y.shape[0]</span><br><span class="line">            n +&#x3D; y.shape[0]</span><br><span class="line"></span><br><span class="line">        if (epoch + 1) % pred_period &#x3D;&#x3D; 0:</span><br><span class="line">            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (</span><br><span class="line">                epoch + 1, math.exp(l_sum &#x2F; n), time.time() - start))</span><br><span class="line">            for prefix in prefixes:</span><br><span class="line">                print(&#39; -&#39;, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class="line">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>

<h5 id="训练模型并创作歌词"><a href="#训练模型并创作歌词" class="headerlink" title="训练模型并创作歌词"></a>训练模型并创作歌词</h5><p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p>
<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta &#x3D; 250, 35, 32, 1e2, 1e-2</span><br><span class="line">pred_period, pred_len, prefixes &#x3D; 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]</span><br></pre></td></tr></table></figure>

<p>下面采用随机采样训练模型并创作歌词。</p>
<p>In [13]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, True, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line">epoch 50, perplexity 65.808092, time 0.78 sec</span><br><span class="line"> - 分开 我想要这样 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我</span><br><span class="line"> - 不分开 别颗去 一颗两 三颗四 一颗四 三颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一颗四 一</span><br><span class="line">epoch 100, perplexity 9.794889, time 0.72 sec</span><br><span class="line"> - 分开 一直在美留 谁在它停 在小村外的溪边 默默等  什么 旧你在依旧 我有儿有些瘦 世色我遇见你是一场</span><br><span class="line"> - 不分开吗 我不能再想 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 我不 </span><br><span class="line">epoch 150, perplexity 2.772557, time 0.80 sec</span><br><span class="line"> - 分开 有直在不妥 有话它停留 蜥蝪横怕落 不爽就 旧怪堂 是属于依 心故之 的片段 有一些风霜 老唱盘 </span><br><span class="line"> - 不分开吗 然后将过不 我慢 失些  如  静里回的太快 想通 却又再考倒我 说散 你想很久了吧?的我 从等</span><br><span class="line">epoch 200, perplexity 1.601744, time 0.73 sec</span><br><span class="line"> - 分开 那只都它满在我面妈 捏成你的形状啸而过 或愿说在后能 让梭时忆对着轻轻 我想就这样牵着你的手不放开</span><br><span class="line"> - 不分开期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 不是</span><br><span class="line">epoch 250, perplexity 1.323342, time 0.78 sec</span><br><span class="line"> - 分开 出愿段的哭咒的天蛦丘好落 拜托当血穿永杨一定的诗篇 我给你的爱写在西元前 深埋在美索不达米亚平原 </span><br><span class="line"> - 不分开扫把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 我来了我 在我感外的溪边河口默默</span><br></pre></td></tr></table></figure>

<p>接下来采用相邻采样训练模型并创作歌词。</p>
<p>In [14]:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class="line">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                      char_to_idx, False, num_epochs, num_steps, lr,</span><br><span class="line">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                      prefixes)</span><br><span class="line">epoch 50, perplexity 60.294393, time 0.74 sec</span><br><span class="line"> - 分开 我想要你想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我</span><br><span class="line"> - 不分开 我想要你 你有了 别不我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我</span><br><span class="line">epoch 100, perplexity 7.141162, time 0.72 sec</span><br><span class="line"> - 分开 我已要再爱 我不要再想 我不 我不 我不要再想 我不 我不 我不要 爱情我的见快就像龙卷风 离能开</span><br><span class="line"> - 不分开柳 你天黄一个棍 后知哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 </span><br><span class="line">epoch 150, perplexity 2.090277, time 0.73 sec</span><br><span class="line"> - 分开 我已要这是你在著 不想我都做得到 但那个人已经不是我 没有你在 我却多难熬  没有你在我有多难熬多</span><br><span class="line"> - 不分开觉 你已经离 我想再好 这样心中 我一定带我 我的完空 不你是风 一一彩纵 在人心中 我一定带我妈走</span><br><span class="line">epoch 200, perplexity 1.305391, time 0.77 sec</span><br><span class="line"> - 分开 我已要这样牵看你的手 它一定实现它一定像现 载著你 彷彿载著阳光 不管到你留都是晴天 蝴蝶自在飞力</span><br><span class="line"> - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生</span><br><span class="line">epoch 250, perplexity 1.230800, time 0.79 sec</span><br><span class="line"> - 分开 我不要 是你看的太快了悲慢 担心今手身会大早 其么我也睡不着  昨晚梦里你来找 我才  原来我只想</span><br><span class="line"> - 不分开觉 你在经离开我 不知不觉 你知了有节奏 后知后觉 后知了一个秋 后知后觉 我该好好生活 我该好好生</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2020 Maxwell
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
            
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>